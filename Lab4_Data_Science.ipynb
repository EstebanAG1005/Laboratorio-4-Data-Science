{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EstebanAG1005/Laboratorio-4-Data-Science/blob/main/Lab4_Data_Science.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0g_k0s1nSp9V"
      },
      "source": [
        "### 1. Importación de Datos:\n",
        "Utilice el conjunto de datos IMDB proporcionado por Keras. pero\n",
        "esta vez, en lugar de utilizar sólo las 20.000 palabras más frecuentes, utilice las 50.000\n",
        "palabras más frecuentes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "JM7sqn4MSp9Z"
      },
      "outputs": [],
      "source": [
        "# Importando las bibliotecas y funciones necesarias.\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding\n",
        "from tensorflow.keras.layers import LSTM\n",
        "from tensorflow.keras.datasets import imdb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jITJKu-1Sp9a",
        "outputId": "b784c82e-bcd1-4836-8e39-ffcebd1bc586"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cargando los datos...\n"
          ]
        }
      ],
      "source": [
        "# Cargando el conjunto de datos IMDB con las 50,000 palabras más frecuentes.\n",
        "print('Cargando los datos...')\n",
        "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=50000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_jUfsQNW9KLw"
      },
      "source": [
        "### 2. Pre-procesamiento:\n",
        "\n",
        "• Secuencie y rellene las críticas para que todas tengan una longitud uniforme.\n",
        "\n",
        "• De las críticas, extraiga características (features) adicionales, por ejemplo. la\n",
        "longitud de la crítica, la proporción de palabras positivas/negativas y cualquier\n",
        "otra que considere pueda ser útil."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "PRbPFqHdSp9b"
      },
      "outputs": [],
      "source": [
        "# Realizando preprocesamiento en los datos: ajustando la longitud de las críticas a un tamaño uniforme.\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "max_sequence_length = 500  # Longitud máxima de la secuencia\n",
        "X_train = sequence.pad_sequences(X_train, maxlen = max_sequence_length)\n",
        "X_test = sequence.pad_sequences(X_test, maxlen = max_sequence_length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "v0rSvGxsln8o"
      },
      "outputs": [],
      "source": [
        "# Obteniendo el índice de palabras del conjunto de datos IMDB.\n",
        "word_to_id = imdb.get_word_index()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ntcZtqAOlpuZ"
      },
      "outputs": [],
      "source": [
        "# Invertiendo el índice de palabras para obtener una correspondencia del ID de la palabra a la palabra real.\n",
        "id_to_word = {value: key for key, value in word_to_id.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "61QA_tDfSp9c"
      },
      "outputs": [],
      "source": [
        "# Función para calcular la proporción de palabras positivas a negativas en una crítica.\n",
        "def calcular_ratio_positivo_negativo(sequence):\n",
        "    palabras_positivas = [\"good\", \"excellent\", \"wonderful\"]  # IMDB está en inglés\n",
        "    palabras_negativas = [\"bad\", \"terrible\", \"horrible\"]\n",
        "\n",
        "    # Convertir secuencia a palabras\n",
        "    words = [id_to_word.get(i, '') for i in sequence]\n",
        "\n",
        "    # Contar palabras positivas y negativas en el texto\n",
        "    num_palabras_positivas = sum(1 for palabra in words if palabra in palabras_positivas)\n",
        "    num_palabras_negativas = sum(1 for palabra in words if palabra in palabras_negativas)\n",
        "\n",
        "    # Calcular la proporción\n",
        "    if num_palabras_negativas == 0:\n",
        "        return num_palabras_positivas\n",
        "    else:\n",
        "        return num_palabras_positivas / num_palabras_negativas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "zlnRe3YQSp9c"
      },
      "outputs": [],
      "source": [
        "# Extracción de características adicionales como la longitud de la crítica y la proporción de palabras positivas/negativas.\n",
        "import numpy as np\n",
        "\n",
        "# Longitud de la crítica\n",
        "lengths_train = np.array([len(seq) for seq in X_train])\n",
        "lengths_test = np.array([len(seq) for seq in X_test])\n",
        "\n",
        "# Proporción de palabras positivas/negativas (esto debe calcularse con datos previamente etiquetados)\n",
        "positive_words_ratio_train = np.array([calcular_ratio_positivo_negativo(seq) for seq in X_train])\n",
        "positive_words_ratio_test = np.array([calcular_ratio_positivo_negativo(seq) for seq in X_test])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "d0NBDkhbbY7I"
      },
      "outputs": [],
      "source": [
        "from textblob import TextBlob\n",
        "\n",
        "# Cálculo de polaridad y subjetividad\n",
        "def extract_polarity_subj(sequence):\n",
        "    words = [id_to_word.get(i, '') for i in sequence]\n",
        "    review = ' '.join(words)\n",
        "    blob = TextBlob(review)\n",
        "    return blob.sentiment.polarity, blob.sentiment.subjectivity\n",
        "\n",
        "polarity_train, subj_train = zip(*[extract_polarity_subj(seq) for seq in X_train])\n",
        "polarity_test, subj_test = zip(*[extract_polarity_subj(seq) for seq in X_test])\n",
        "\n",
        "# Convertir a arrays de numpy y redimensionar\n",
        "polarity_train = np.array(polarity_train).reshape(-1, 1)\n",
        "polarity_test = np.array(polarity_test).reshape(-1, 1)\n",
        "subj_train = np.array(subj_train).reshape(-1, 1)\n",
        "subj_test = np.array(subj_test).reshape(-1, 1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "VVID85AubdDB"
      },
      "outputs": [],
      "source": [
        "# Celda después de la celda anterior:\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Convertir secuencias en textos para el análisis de n-gramas\n",
        "reviews_train = [\" \".join([id_to_word.get(i, '') for i in seq]) for seq in X_train]\n",
        "reviews_test = [\" \".join([id_to_word.get(i, '') for i in seq]) for seq in X_test]\n",
        "\n",
        "# Aplicar TF-IDF solo en bigramas y trigramas\n",
        "vectorizer = TfidfVectorizer(ngram_range=(2, 3), max_features=5000)  # limitando a 5000 características por cuestiones de memoria y rendimiento\n",
        "X_train_ngrams = vectorizer.fit_transform(reviews_train)\n",
        "X_test_ngrams = vectorizer.transform(reviews_test)\n",
        "\n",
        "# Convertir matrices dispersas en densas\n",
        "X_train_ngrams = X_train_ngrams.todense()\n",
        "X_test_ngrams = X_test_ngrams.todense()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "v6NxE4XwSp9c"
      },
      "outputs": [],
      "source": [
        "# Redimensionando las características extraídas para que tengan la forma adecuada.\n",
        "lengths_train = lengths_train.reshape(-1, 1)\n",
        "lengths_test = lengths_test.reshape(-1, 1)\n",
        "positive_words_ratio_train = positive_words_ratio_train.reshape(-1, 1)\n",
        "positive_words_ratio_test = positive_words_ratio_test.reshape(-1, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Z8FsX1IcSp9d"
      },
      "outputs": [],
      "source": [
        "# Combinando las características adicionales.\n",
        "additional_features_train = np.hstack([lengths_train, positive_words_ratio_train, polarity_train, subj_train, X_train_ngrams])\n",
        "additional_features_test = np.hstack([lengths_test, positive_words_ratio_test, polarity_test, subj_test, X_test_ngrams])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aYnn5jP-9KL0"
      },
      "source": [
        "### 3. Modelo:\n",
        "\n",
        "• Cree un modelo LSTM que acepte las características (features) adicionales junto\n",
        "con la secuencia de palabras.\n",
        "\n",
        "• Intente usar una arquitectura más compleja, incorporando más capas LSTM, capas\n",
        "de Dropout para la regularización y tal vez alguna capa densamente conectada\n",
        "después de la LSTM. (ver también la referencia al final de este documento)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "143jDmPJSp9d",
        "outputId": "fcc12ddd-8a18-4638-88d7-e2aa9814c7ac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/15\n",
            "391/391 - 115s - loss: 0.6899 - accuracy: 0.6438 - val_loss: 0.4097 - val_accuracy: 0.8391 - lr: 0.0010 - 115s/epoch - 295ms/step\n",
            "Epoch 2/15\n",
            "391/391 - 70s - loss: 0.3271 - accuracy: 0.8647 - val_loss: 0.3152 - val_accuracy: 0.8708 - lr: 0.0010 - 70s/epoch - 178ms/step\n",
            "Epoch 3/15\n",
            "391/391 - 56s - loss: 0.1709 - accuracy: 0.9377 - val_loss: 0.2989 - val_accuracy: 0.8714 - lr: 0.0010 - 56s/epoch - 144ms/step\n",
            "Epoch 4/15\n",
            "391/391 - 50s - loss: 0.1029 - accuracy: 0.9651 - val_loss: 0.3872 - val_accuracy: 0.8692 - lr: 0.0010 - 50s/epoch - 127ms/step\n",
            "Epoch 5/15\n",
            "391/391 - 48s - loss: 0.0648 - accuracy: 0.9802 - val_loss: 0.4317 - val_accuracy: 0.8698 - lr: 0.0010 - 48s/epoch - 124ms/step\n",
            "Epoch 6/15\n",
            "391/391 - 46s - loss: 0.0266 - accuracy: 0.9937 - val_loss: 0.4592 - val_accuracy: 0.8681 - lr: 2.0000e-04 - 46s/epoch - 119ms/step\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7d6c8fbaee60>"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, Dropout, Bidirectional\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import concatenate\n",
        "\n",
        "\n",
        "# Entrada de secuencia\n",
        "input_seq = Input(shape=(max_sequence_length,))\n",
        "embedding = Embedding(50000, 128)(input_seq)\n",
        "\n",
        "# Capa LSTM bidireccional\n",
        "bi_lstm = Bidirectional(LSTM(128, return_sequences=True, dropout=0.3))(embedding)\n",
        "lstm = LSTM(64, dropout=0.2)(bi_lstm)\n",
        "\n",
        "# Entrada de características adicionales\n",
        "# Obtener la forma de las características adicionales (el segundo valor es el número de características)\n",
        "num_additional_features = additional_features_train.shape[1]\n",
        "\n",
        "# Modificar la entrada para las características adicionales en el modelo para que coincida con la nueva forma\n",
        "input_features = Input(shape=(num_additional_features,))\n",
        "\n",
        "# Combinar ambas entradas\n",
        "merged = concatenate([lstm, input_features])\n",
        "dropout = Dropout(0.2)(merged)\n",
        "dense1 = Dense(64, activation='relu')(dropout)\n",
        "dense2 = Dense(32, activation='relu')(dense1)\n",
        "output = Dense(1, activation='sigmoid')(dense2)\n",
        "\n",
        "# Compilación\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "model = Model(inputs=[input_seq, input_features], outputs=output)\n",
        "model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "\n",
        "# Callbacks\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3)\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=2, min_lr=0.0001)\n",
        "\n",
        "# Entrenar el modelo con Early Stopping y ReduceLROnPlateau\n",
        "model.fit([X_train, additional_features_train], y_train,\n",
        "          batch_size=64,\n",
        "          epochs=15,\n",
        "          verbose=2,\n",
        "          validation_data=([X_test, additional_features_test], y_test),\n",
        "          callbacks=[early_stopping, reduce_lr])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v6lcqS6p9KL1"
      },
      "source": [
        "### 4. Entrenamiento y Evaluación:\n",
        "Entrene su modelo con el conjunto de datos de entrenamiento y evalúe su desempeño con el conjunto de datos de prueba."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "kkpTLRox9KL1"
      },
      "outputs": [],
      "source": [
        "# # Entrenar el modelo con callback de Early Stopping\n",
        "# model.fit([X_train, additional_features_train], y_train,\n",
        "#           batch_size=64,  # Aumentado el tamaño del lote para una convergencia más estable\n",
        "#           epochs=15,\n",
        "#           verbose=2,\n",
        "#           validation_data=([X_test, additional_features_test], y_test),\n",
        "#           callbacks=[early_stopping])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ywrqCjN8Sp9e",
        "outputId": "a003f864-55b7-41b9-a331-504c4e0e7797"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "782/782 - 16s - loss: 0.4592 - accuracy: 0.8681 - 16s/epoch - 20ms/step\n",
            "Pérdida de la Prueba: 0.45924100279808044\n",
            "Exactitud de la Prueba (Test accuracy): 0.8680800199508667\n"
          ]
        }
      ],
      "source": [
        "# Evaluando el modelo y mostrando resultados.\n",
        "perdida, exactitud = model.evaluate([X_test, additional_features_test], y_test,\n",
        "                                    batch_size = 32,\n",
        "                                    verbose = 2)\n",
        "print('Pérdida de la Prueba:', perdida)\n",
        "print('Exactitud de la Prueba (Test accuracy):', exactitud)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KVnY8CYCSp9e",
        "outputId": "53456238-4aed-4f19-c227-c83ea78da0fc"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ],
      "source": [
        "# Guardando el modelo entrenado.\n",
        "model.save(\"Sentimiento.h5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "YXTipkfhSp9e"
      },
      "outputs": [],
      "source": [
        "# Use the default parameters to keras.datasets.imdb.load_data\n",
        "start_char = 1\n",
        "oov_char = 2\n",
        "index_from = 3\n",
        "# Retrieve the training sequences.\n",
        "(X_train, _), _ = tf.keras.datasets.imdb.load_data(\n",
        "    start_char=start_char, oov_char=oov_char, index_from=index_from\n",
        ")\n",
        "# Retrieve the word index file mapping words to indices\n",
        "word_index = tf.keras.datasets.imdb.get_word_index()\n",
        "# Reverse the word index to obtain a dict mapping indices to words\n",
        "# And add `index_from` to indices to sync with `x_train`\n",
        "inverted_word_index = dict(\n",
        "    (i + index_from, word) for (word, i) in word_index.items()\n",
        ")\n",
        "# Update `inverted_word_index` to include `start_char` and `oov_char`\n",
        "inverted_word_index[start_char] = \"[START]\"\n",
        "inverted_word_index[oov_char] = \"[OOV]\"\n",
        "# Decode the first sequence in the dataset\n",
        "decoded_sequence = \" \".join(inverted_word_index[i] for i in X_train[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        },
        "id": "cmTmuu3eSp9e",
        "outputId": "c8162115-c58d-42d7-a605-794afe4d1173"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"[START] this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert redford's is an amazing actor and now the same being director norman's father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for retail and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also congratulations to the two little boy's that played the part's of norman and paul they were just brilliant children are often left out of the praising list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was shared with us all\""
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Mostrando la secuencia decodificada.\n",
        "decoded_sequence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5. Informe:\n",
        "\n",
        "\n",
        "#### **Características adicionales seleccionadas y la razón de su elección**:\n",
        "Se seleccionaron varias características adicionales para mejorar la capacidad del modelo de identificar el sentimiento subyacente en las críticas de películas:\n",
        "\n",
        "1. **Longitud de la crítica**: La longitud puede ser indicativa de la naturaleza de la revisión. Las críticas extremadamente cortas o largas podrían tener un tono o sentimiento característico.\n",
        "2. **Proporción de palabras positivas/negativas**: Esta característica permite cuantificar la proporción de palabras con connotaciones positivas frente a negativas en una revisión, proporcionando una métrica directa del sentimiento.\n",
        "3. **Polaridad y Subjetividad**: Estos son indicadores directos del sentimiento y la objetividad de una revisión, respectivamente. Ayudan a discernir entre revisiones objetivas y opiniones sesgadas.\n",
        "4. **Análisis de n-gramas**: Captura combinaciones de palabras que pueden tener significados distintos de sus componentes individuales, permitiendo al modelo identificar contextos y frases específicas que son indicativos del sentimiento.\n",
        "\n",
        "#### **Arquitectura del modelo y las razones detrás de sus elecciones**:\n",
        "La arquitectura está diseñada para manejar datos secuenciales (texto) y combinarlos con características adicionales para una predicción robusta.\n",
        "\n",
        "- **Capa de Incrustación (Embedding)**: Transforma los índices de palabras en vectores densos de tamaño fijo. Estos vectores contienen información semántica sobre las palabras.\n",
        "  \n",
        "- **LSTM Bidireccional**: Esta capa captura patrones en los datos tanto en las direcciones adelante como atrás en el tiempo o secuencia. Es especialmente útil para el análisis de sentimientos, ya que el contexto puede venir antes o después de una palabra en particular.\n",
        "\n",
        "- **Capas Densas**: Se utilizan para aprender patrones a partir de características combinadas y hacer la clasificación final. Las capas densas pueden capturar interacciones no lineales.\n",
        "\n",
        "- **Dropout y Regularización L2**: Ayudan a prevenir el sobreajuste, asegurando que el modelo generalice bien en datos no vistos.\n",
        "\n",
        "La elección de esta arquitectura se basa en el deseo de combinar información secuencial (texto) con características generales para obtener una comprensión más completa del sentimiento en una revisión.\n",
        "\n",
        "#### **Resultados obtenidos y una breve comparación con el modelo simple del ejercicio anterior**:\n",
        "El modelo mejorado logra un accuracy del 86.8% lo cual es una precisión notable, muestra una mejora aunque no tan significativa en comparación con el modelo anterior que tenía un 81-82%. Las características adicionales y las modificaciones en la arquitectura del modelo se realizaron con la intención de capturar más patrones y mejorar la precisión. Sin embargo, esto demuestra que el análisis de sentimientos es una tarea desafiante y que pequeñas modificaciones pueden no llevar siempre a grandes mejoras. Es crucial considerar la complejidad adicional frente a las ganancias obtenidas al hacer ajustes en el modelo.\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
