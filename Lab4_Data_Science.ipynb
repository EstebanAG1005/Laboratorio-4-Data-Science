{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0g_k0s1nSp9V"
      },
      "source": [
        "### 1. Importación de Datos:\n",
        "Utilice el conjunto de datos IMDB proporcionado por Keras. pero\n",
        "esta vez, en lugar de utilizar sólo las 20.000 palabras más frecuentes, utilice las 50.000\n",
        "palabras más frecuentes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JM7sqn4MSp9Z"
      },
      "outputs": [],
      "source": [
        "# Importando las bibliotecas y funciones necesarias.\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding\n",
        "from tensorflow.keras.layers import LSTM\n",
        "from tensorflow.keras.datasets import imdb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jITJKu-1Sp9a",
        "outputId": "acde12a5-2ef1-4c6e-c6fc-a96f7cf6f61c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cargando los datos...\n"
          ]
        }
      ],
      "source": [
        "# Cargando el conjunto de datos IMDB con las 50,000 palabras más frecuentes.\n",
        "print('Cargando los datos...')\n",
        "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=50000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_jUfsQNW9KLw"
      },
      "source": [
        "### 2. Pre-procesamiento:\n",
        "\n",
        "• Secuencie y rellene las críticas para que todas tengan una longitud uniforme.\n",
        "\n",
        "• De las críticas, extraiga características (features) adicionales, por ejemplo. la\n",
        "longitud de la crítica, la proporción de palabras positivas/negativas y cualquier\n",
        "otra que considere pueda ser útil."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PRbPFqHdSp9b"
      },
      "outputs": [],
      "source": [
        "# Realizando preprocesamiento en los datos: ajustando la longitud de las críticas a un tamaño uniforme.\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "max_sequence_length = 500  # Longitud máxima de la secuencia\n",
        "X_train = sequence.pad_sequences(X_train, maxlen = max_sequence_length)\n",
        "X_test = sequence.pad_sequences(X_test, maxlen = max_sequence_length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v0rSvGxsln8o"
      },
      "outputs": [],
      "source": [
        "# Obteniendo el índice de palabras del conjunto de datos IMDB.\n",
        "word_to_id = imdb.get_word_index()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ntcZtqAOlpuZ"
      },
      "outputs": [],
      "source": [
        "# Invertiendo el índice de palabras para obtener una correspondencia del ID de la palabra a la palabra real.\n",
        "id_to_word = {value: key for key, value in word_to_id.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "61QA_tDfSp9c"
      },
      "outputs": [],
      "source": [
        "# Función para calcular la proporción de palabras positivas a negativas en una crítica.\n",
        "def calcular_ratio_positivo_negativo(sequence):\n",
        "    palabras_positivas = [\"good\", \"excellent\", \"wonderful\"]  # IMDB está en inglés\n",
        "    palabras_negativas = [\"bad\", \"terrible\", \"horrible\"]\n",
        "\n",
        "    # Convertir secuencia a palabras\n",
        "    words = [id_to_word.get(i, '') for i in sequence]\n",
        "\n",
        "    # Contar palabras positivas y negativas en el texto\n",
        "    num_palabras_positivas = sum(1 for palabra in words if palabra in palabras_positivas)\n",
        "    num_palabras_negativas = sum(1 for palabra in words if palabra in palabras_negativas)\n",
        "\n",
        "    # Calcular la proporción\n",
        "    if num_palabras_negativas == 0:\n",
        "        return num_palabras_positivas\n",
        "    else:\n",
        "        return num_palabras_positivas / num_palabras_negativas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zlnRe3YQSp9c"
      },
      "outputs": [],
      "source": [
        "# Extracción de características adicionales como la longitud de la crítica y la proporción de palabras positivas/negativas.\n",
        "import numpy as np\n",
        "\n",
        "# Longitud de la crítica\n",
        "lengths_train = np.array([len(seq) for seq in X_train])\n",
        "lengths_test = np.array([len(seq) for seq in X_test])\n",
        "\n",
        "# Proporción de palabras positivas/negativas (esto debe calcularse con datos previamente etiquetados)\n",
        "positive_words_ratio_train = np.array([calcular_ratio_positivo_negativo(seq) for seq in X_train])\n",
        "positive_words_ratio_test = np.array([calcular_ratio_positivo_negativo(seq) for seq in X_test])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v6NxE4XwSp9c"
      },
      "outputs": [],
      "source": [
        "# Redimensionando las características extraídas para que tengan la forma adecuada.\n",
        "lengths_train = lengths_train.reshape(-1, 1)\n",
        "lengths_test = lengths_test.reshape(-1, 1)\n",
        "positive_words_ratio_train = positive_words_ratio_train.reshape(-1, 1)\n",
        "positive_words_ratio_test = positive_words_ratio_test.reshape(-1, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z8FsX1IcSp9d"
      },
      "outputs": [],
      "source": [
        "# Combinando las características adicionales.\n",
        "additional_features_train = np.hstack([lengths_train, positive_words_ratio_train])\n",
        "additional_features_test = np.hstack([lengths_test, positive_words_ratio_test])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aYnn5jP-9KL0"
      },
      "source": [
        "### 3. Modelo:\n",
        "\n",
        "• Cree un modelo LSTM que acepte las características (features) adicionales junto\n",
        "con la secuencia de palabras.\n",
        "\n",
        "• Intente usar una arquitectura más compleja, incorporando más capas LSTM, capas\n",
        "de Dropout para la regularización y tal vez alguna capa densamente conectada\n",
        "después de la LSTM. (ver también la referencia al final de este documento)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "143jDmPJSp9d"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, Dropout, concatenate\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# Entrada de secuencia\n",
        "input_seq = Input(shape=(max_sequence_length,))\n",
        "embedding = Embedding(50000, 128)(input_seq)\n",
        "\n",
        "# Primera capa LSTM con dropout\n",
        "lstm1 = LSTM(128, return_sequences=True, dropout=0.3)(embedding)\n",
        "\n",
        "# Segunda capa LSTM con dropout y conexión residual\n",
        "lstm2 = LSTM(64, return_sequences=True, dropout=0.2)(lstm1)\n",
        "residual_connection = concatenate([lstm1, lstm2])  # Conexión residual\n",
        "lstm2_res = LSTM(64, dropout=0.2)(residual_connection)\n",
        "\n",
        "# Entrada de características adicionales\n",
        "input_features = Input(shape=(2,))  # Aquí asumimos que tenemos 2 características adicionales\n",
        "\n",
        "# Combinar ambas entradas\n",
        "merged = concatenate([lstm2_res, input_features])\n",
        "\n",
        "# Agregar más capas\n",
        "dropout = Dropout(0.2)(merged)\n",
        "dense1 = Dense(64, activation='relu')(dropout)\n",
        "dense2 = Dense(32, activation='relu')(dense1)\n",
        "output = Dense(1, activation='sigmoid')(dense2)\n",
        "\n",
        "# Crear y compilar el modelo con un optimizador y tasa de aprendizaje ajustada\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)  # Puedes experimentar con esta tasa\n",
        "model = Model(inputs=[input_seq, input_features], outputs=output)\n",
        "model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "\n",
        "# Implementación del Early Stopping\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v6lcqS6p9KL1"
      },
      "source": [
        "### 4. Entrenamiento y Evaluación:\n",
        "Entrene su modelo con el conjunto de datos de entrenamiento y evalúe su desempeño con el conjunto de datos de prueba."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kkpTLRox9KL1",
        "outputId": "0f72647a-0f79-48c7-e385-80f2235fb2c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "391/391 - 89s - loss: 1.1757 - accuracy: 0.5215 - val_loss: 0.5726 - val_accuracy: 0.6969 - 89s/epoch - 229ms/step\n",
            "Epoch 2/15\n",
            "391/391 - 63s - loss: 0.4251 - accuracy: 0.8139 - val_loss: 0.3443 - val_accuracy: 0.8510 - 63s/epoch - 161ms/step\n",
            "Epoch 3/15\n",
            "391/391 - 50s - loss: 0.2820 - accuracy: 0.8949 - val_loss: 0.3706 - val_accuracy: 0.8615 - 50s/epoch - 128ms/step\n",
            "Epoch 4/15\n",
            "391/391 - 46s - loss: 0.1648 - accuracy: 0.9417 - val_loss: 0.3740 - val_accuracy: 0.8684 - 46s/epoch - 117ms/step\n",
            "Epoch 5/15\n",
            "391/391 - 39s - loss: 0.1089 - accuracy: 0.9652 - val_loss: 0.4371 - val_accuracy: 0.8620 - 39s/epoch - 101ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7eec20fad0f0>"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "# Entrenar el modelo con callback de Early Stopping\n",
        "model.fit([X_train, additional_features_train], y_train,\n",
        "          batch_size=64,  # Aumentado el tamaño del lote para una convergencia más estable\n",
        "          epochs=15,\n",
        "          verbose=2,\n",
        "          validation_data=([X_test, additional_features_test], y_test),\n",
        "          callbacks=[early_stopping])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ywrqCjN8Sp9e",
        "outputId": "cca0a471-583c-45ab-f6da-b04651776348"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "782/782 - 15s - loss: 0.4371 - accuracy: 0.8620 - 15s/epoch - 19ms/step\n",
            "Pérdida de la Prueba: 0.43710583448410034\n",
            "Exactitud de la Prueba (Test accuracy): 0.8620399832725525\n"
          ]
        }
      ],
      "source": [
        "# Evaluando el modelo y mostrando resultados.\n",
        "perdida, exactitud = model.evaluate([X_test, additional_features_test], y_test,\n",
        "                                    batch_size = 32,\n",
        "                                    verbose = 2)\n",
        "print('Pérdida de la Prueba:', perdida)\n",
        "print('Exactitud de la Prueba (Test accuracy):', exactitud)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KVnY8CYCSp9e"
      },
      "outputs": [],
      "source": [
        "# Guardando el modelo entrenado.\n",
        "model.save(\"Sentimiento.h5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YXTipkfhSp9e"
      },
      "outputs": [],
      "source": [
        "# Use the default parameters to keras.datasets.imdb.load_data\n",
        "start_char = 1\n",
        "oov_char = 2\n",
        "index_from = 3\n",
        "# Retrieve the training sequences.\n",
        "(X_train, _), _ = tf.keras.datasets.imdb.load_data(\n",
        "    start_char=start_char, oov_char=oov_char, index_from=index_from\n",
        ")\n",
        "# Retrieve the word index file mapping words to indices\n",
        "word_index = tf.keras.datasets.imdb.get_word_index()\n",
        "# Reverse the word index to obtain a dict mapping indices to words\n",
        "# And add `index_from` to indices to sync with `x_train`\n",
        "inverted_word_index = dict(\n",
        "    (i + index_from, word) for (word, i) in word_index.items()\n",
        ")\n",
        "# Update `inverted_word_index` to include `start_char` and `oov_char`\n",
        "inverted_word_index[start_char] = \"[START]\"\n",
        "inverted_word_index[oov_char] = \"[OOV]\"\n",
        "# Decode the first sequence in the dataset\n",
        "decoded_sequence = \" \".join(inverted_word_index[i] for i in X_train[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        },
        "id": "cmTmuu3eSp9e",
        "outputId": "663ac933-44e4-4e58-fa90-fb95bd050c07"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"[START] this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert redford's is an amazing actor and now the same being director norman's father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for retail and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also congratulations to the two little boy's that played the part's of norman and paul they were just brilliant children are often left out of the praising list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was shared with us all\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "# Mostrando la secuencia decodificada.\n",
        "decoded_sequence"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}